identity:
  name: "content-safety-container"
  author: "jiqing"
  label:
    en_US: "Content moderation"
    zh_Hans: "内容审核"
description:
  human:
    en_US: "Analyze text and images to detect harmful content using Azure AI Content Safety Container."
    zh_Hans: "使用 Azure AI Content Safety Container 分析文本和图像，以检测有害内容。"
  llm: "This tool analyzes text and images for potential harmful content using Azure AI Content Safety API. It checks for categories like hate, sexual, violence, and self-harm content."
parameters:
  - name: text
    type: string
    required: false
    label:
      en_US: Text to Analyze
      zh_Hans: 待分析文本
    human_description:
      en_US: "The text content to analyze for safety"
      zh_Hans: "要进行有害内容分析的文本内容，一般引用 sys.query"
    llm_description: "The text content that needs to be analyzed for potential harmful content"
    form: llm
  - name: image
    type: files
    required: false
    label:
      en_US: Images to Analyze
      zh_Hans: 待分析图片
    human_description:
      en_US: "The image content to analyze for safety"
      zh_Hans: "要进行有害内容分析的图片内容，一般引用 sys.files"
    llm_description: "The image content that needs to be analyzed for potential harmful content"
    form: llm
  - name: blocklist_names
    type: string
    required: false
    label:
      en_US: Text Blocklist Names
      zh_Hans: 文本黑名单名称
    human_description:
      en_US: "Comma-separated list of blocklist names to check against"
      zh_Hans: "在有害内容分析中使用的黑名单名称，多个用逗号分隔"
    llm_description: "Optional comma-separated list of blocklist names to check the text against"
    form: llm
  - name: halt_on_blocklist_hit
    type: boolean
    required: false
    label:
      en_US: Halt on Blocklist Hit
      zh_Hans: 命中黑名单时停止
    human_description:
      en_US: "Whether to halt processing when blocklist is hit"
      zh_Hans: "设置为 true 时，若命中黑名单，则不会对有害内容进行进一步分析。设置为 false 时，无论是命中发黑名单，都会执行所有有害内容分析"
    llm_description: "Whether to halt the analysis when text matches a blocklist entry"
    form: llm
extra:
  python:
    source: tools/content-safety-container.py
output_schema:
  type: object
  properties:
    CheckResult:
      type: string
      description: "Content safety check result: ALLOW, DENY, or ERROR"
    Details:
      type: string
      description: "Detailed information when content is denied or error occurs"
